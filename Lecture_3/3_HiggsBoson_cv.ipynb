{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交叉验证 for Kaggle的Higgs Boson竞赛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始出处： https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "竞赛官网：https://www.kaggle.com/c/higgs-boson/\n",
    "\n",
    "希格斯玻色子（英语：Higgs boson）是标准模型里的一种基本粒子，是因物理学者彼得·希格斯而命名。\n",
    "2012年7月4日，欧洲核子研究组织（CERN）宣布，LHC的紧凑渺子线圈（CMS）探测到质量为125.3±0.6GeV的新玻色子（超过背景期望值4.9个标准差），超环面仪器（ATLAS）测量到质量为126.5GeV的新玻色子（5个标准差），这两种粒子极像希格斯玻色子。\n",
    "2013年3月14日，欧洲核子研究组织发表新闻稿正式宣布，先前探测到的新粒子暂时被确认是希格斯玻色子，具有零自旋与偶宇称，这是希格斯玻色子应该具有的两种基本性质，但有一部分实验结果不尽符合理论预测，更多数据仍在等待处理与分析。\n",
    "2013年10月8日，因为“次原子粒子质量的生成机制理论，促进了人类对这方面的理解，并且最近由欧洲核子研究组织属下大型强子对撞机的超环面仪器及紧凑μ子线圈探测器发现的基本粒子证实”，弗朗索瓦·恩格勒、彼得·希格斯荣获2013年诺贝尔物理学奖。\n",
    "\n",
    "一个粒子的重要特点是它在其他粒子之后延迟多少。CERN用ATLAS进行物理实验来寻找新粒子。实验最近发现一个 Higgs boson延迟在两个tau粒子出现，但是该延迟只是淹没在背景噪声中的小信号。\n",
    "\n",
    "该竞赛的目的是利用机器学习方法，提高ATLAS实验发现粒子的显著性。竞赛无需粒子物理的背景知识（解决实际问题时背景知识在很大程度上还是有用的）。竞赛数据是根据ATLAS检测到的事件的特征合成的数据，竞赛任务是将事件分类为\"tau tau decay of a Higgs boson\" 或 \"background\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training.csv：训练集包含250000个事件，每个事件有一个ID，30个特征，权重，和标签。\n",
    "test.csv：测试数据包含550000事件，每个事件包含一个ID和30个特征。\n",
    "\n",
    "所有变量都是floating point类型，除了PRI_jet_num 为integer\n",
    "以PRI  （PRImitives) 为的前缀特征为检测器测量得到的关于bunch collision“原始” 数据.\n",
    "以DER （ DERived)为ATLAS的物理学家选择的根据原始特征计算得到的数据。\n",
    "缺失数据记为 −999.0, 与所有特征的正常值不同。\n",
    "\n",
    "特征、权重和标签的具体语意可以查看CERN的技术文档。（竞赛官网有链接）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# this is the example script to use xgboost to train\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading from csv \n"
     ]
    }
   ],
   "source": [
    "test_size = 550000\n",
    "\n",
    "# path to where the data lies\n",
    "dpath = './data/'\n",
    "                     \n",
    "# load in training data, directly use numpy\n",
    "dtrain = np.loadtxt( dpath+'/higgsboson_training.csv', delimiter=',', skiprows=1, converters={32: lambda x:int(x=='s'.encode('utf-8')) } )\n",
    "print ('finish loading from csv ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight statistics: wpos=1522.37, wneg=904200, ratio=593.94\n"
     ]
    }
   ],
   "source": [
    "label  = dtrain[:,32]\n",
    "data   = dtrain[:,1:31]\n",
    "# rescale weight to make it same as test set\n",
    "weight = dtrain[:,31] * float(test_size) / len(label)\n",
    "\n",
    "# 正负样本权重，为训练集中正负样本的比例\n",
    "sum_wpos = sum( weight[i] for i in range(len(label)) if label[i] == 1.0  )\n",
    "sum_wneg = sum( weight[i] for i in range(len(label)) if label[i] == 0.0  )\n",
    "\n",
    "# print weight statistics\n",
    "print ('weight statistics: wpos=%g, wneg=%g, ratio=%g' % ( sum_wpos, sum_wneg, sum_wneg/sum_wpos ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost训练环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据导入DMatix，这样后续训练更快\n",
    "\n",
    "训练参数设置：\n",
    "1. objective[默认reg:linear]：\n",
    "定义需要被最小化的损失函数： \n",
    "        binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。\n",
    "        Higgs Boson竞赛是一个两类分类任务，采用二分类的逻辑回归。\n",
    "        \n",
    "2. scale_pos_weight[默认1]：\n",
    "在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。\n",
    "Higgs Boson竞赛中训练集给出了每个（正／负）样本的权重，所有正／负样本的权重相加，可得到训练集中正负样本的比例。\n",
    "\n",
    "3. eta[默认0.3]\n",
    "为学习率。为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。取值范围为：[0,1]\n",
    "\n",
    "4. max_depth[默认6]\n",
    "定义树的最大深度，这个值也是用来避免过拟合的。max_depth越大，模型越复杂，会学到更具体更局部的样本。\n",
    "典型值：3-10\n",
    "\n",
    "5. eval_metric[默认值取决于objective参数的取值]\n",
    "对于有效数据的度量方法。\n",
    "对于回归问题，默认值是rmse，对于分类问题，默认值是error。\n",
    "典型值有： \n",
    "rmse 均方根误差\n",
    "mae 平均绝对误差\n",
    "logloss 负对数似然函数值\n",
    "error 二分类错误率(阈值为0.5)\n",
    "merror 多分类错误率\n",
    "mlogloss 多分类logloss损失函数\n",
    "auc 曲线下面积（Area Under Curve）：不同阈值下模型的正确率。\n",
    "\n",
    "6. nthread[默认值为最大可能的线程数]\n",
    "这个参数用来进行多线程控制，应当输入系统的核数。\n",
    "如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct xgboost.DMatrix from numpy array, treat -999.0 as missing value\n",
    "dtrain = xgb.DMatrix( data, label=label, missing = -999.0, weight=weight )\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use logistic regression loss, use raw prediction before logistic transformation\n",
    "# since we only need the rank\n",
    "param['objective'] = 'binary:logitraw'\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost模型训练，并保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train函数的参数：\n",
    "xgboost.train(params,dtrain,num_boost_round=10,evals=(),obj=None,feval=None,maximize=False,early_stopping_rounds=None,\n",
    "evals_result=None,verbose_eval=True,learning_rates=None,xgb_model=None)\n",
    "\n",
    "params：\n",
    "这是一个字典，里面包含着训练中的参数关键字和对应的值，形式是params = {‘booster’:’gbtree’,’eta’:0.1}\n",
    "\n",
    "dtrain 训练的数据\n",
    "\n",
    "num_boost_round 这是指提升迭代的个数\n",
    "\n",
    "evals： 这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals = [(dtrain,’train’),(dval,’val’)]或者是evals = [(dtrain,’train’)]\n",
    "\n",
    "本代码中用的是第一种情况，它使得我们可以在训练过程中观察验证集的效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running cross validation, with preprocessing function\n",
      "finish cross validation\n"
     ]
    }
   ],
   "source": [
    "# boost 1000 trees\n",
    "num_round = 1000\n",
    "\n",
    "print ('running cross validation, with preprocessing function')\n",
    "# define the preprocessing function\n",
    "# used to return the preprocessed training, test data, and parameter\n",
    "# we can use this to do weight rescale, etc.\n",
    "# as a example, we try to set scale_pos_weight\n",
    "def fpreproc(dtrain, dtest, param):\n",
    "    label = dtrain.get_label()\n",
    "    ratio = float(np.sum(label == 0)) / np.sum(label==1)\n",
    "    param['scale_pos_weight'] = ratio\n",
    "    wtrain = dtrain.get_weight()\n",
    "    wtest = dtest.get_weight()\n",
    "    sum_weight = sum(wtrain) + sum(wtest)\n",
    "    wtrain *= sum_weight / sum(wtrain)\n",
    "    wtest *= sum_weight / sum(wtest)\n",
    "    dtrain.set_weight(wtrain)\n",
    "    dtest.set_weight(wtest)\n",
    "    return (dtrain, dtest, param)\n",
    "\n",
    "# do cross validation, for each fold\n",
    "# the dtrain, dtest, param will be passed into fpreproc\n",
    "# then the return value of fpreproc will be used to generate\n",
    "# results of that fold\n",
    "cvresult = xgb.cv(param, dtrain, num_round, nfold=5,\n",
    "       metrics={'ams@0.15', 'auc'}, early_stopping_rounds=10, seed = 0, fpreproc = fpreproc)\n",
    "\n",
    "\n",
    "\n",
    "print ('finish cross validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-ams@0.15-mean</th>\n",
       "      <th>test-ams@0.15-std</th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>train-ams@0.15-mean</th>\n",
       "      <th>train-ams@0.15-std</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.909543</td>\n",
       "      <td>0.097884</td>\n",
       "      <td>0.842198</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>4.012500</td>\n",
       "      <td>0.154041</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.236945</td>\n",
       "      <td>0.181689</td>\n",
       "      <td>0.854647</td>\n",
       "      <td>0.010645</td>\n",
       "      <td>4.323271</td>\n",
       "      <td>0.080539</td>\n",
       "      <td>0.855859</td>\n",
       "      <td>0.010494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.242105</td>\n",
       "      <td>0.121839</td>\n",
       "      <td>0.859342</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>4.332760</td>\n",
       "      <td>0.110162</td>\n",
       "      <td>0.860942</td>\n",
       "      <td>0.011385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.307576</td>\n",
       "      <td>0.093919</td>\n",
       "      <td>0.869186</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>4.382181</td>\n",
       "      <td>0.072945</td>\n",
       "      <td>0.870628</td>\n",
       "      <td>0.001795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.357200</td>\n",
       "      <td>0.120168</td>\n",
       "      <td>0.870837</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>4.451794</td>\n",
       "      <td>0.040777</td>\n",
       "      <td>0.872648</td>\n",
       "      <td>0.003224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.382403</td>\n",
       "      <td>0.120171</td>\n",
       "      <td>0.873463</td>\n",
       "      <td>0.003857</td>\n",
       "      <td>4.487194</td>\n",
       "      <td>0.044543</td>\n",
       "      <td>0.875088</td>\n",
       "      <td>0.002343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.392322</td>\n",
       "      <td>0.120804</td>\n",
       "      <td>0.877322</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>4.497964</td>\n",
       "      <td>0.058888</td>\n",
       "      <td>0.878971</td>\n",
       "      <td>0.001477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.420645</td>\n",
       "      <td>0.149904</td>\n",
       "      <td>0.878349</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>4.523607</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.879917</td>\n",
       "      <td>0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.435181</td>\n",
       "      <td>0.163421</td>\n",
       "      <td>0.879478</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>4.532700</td>\n",
       "      <td>0.029385</td>\n",
       "      <td>0.881286</td>\n",
       "      <td>0.001085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.452390</td>\n",
       "      <td>0.158298</td>\n",
       "      <td>0.879849</td>\n",
       "      <td>0.002231</td>\n",
       "      <td>4.581649</td>\n",
       "      <td>0.046796</td>\n",
       "      <td>0.881827</td>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.447331</td>\n",
       "      <td>0.156880</td>\n",
       "      <td>0.880632</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>4.571364</td>\n",
       "      <td>0.072250</td>\n",
       "      <td>0.882651</td>\n",
       "      <td>0.001280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.454632</td>\n",
       "      <td>0.154264</td>\n",
       "      <td>0.881006</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>4.595838</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.883039</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.447309</td>\n",
       "      <td>0.177235</td>\n",
       "      <td>0.884396</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>4.608398</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.006251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.418153</td>\n",
       "      <td>0.166041</td>\n",
       "      <td>0.884807</td>\n",
       "      <td>0.005140</td>\n",
       "      <td>4.591107</td>\n",
       "      <td>0.099510</td>\n",
       "      <td>0.886790</td>\n",
       "      <td>0.006548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.465543</td>\n",
       "      <td>0.156981</td>\n",
       "      <td>0.887643</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>4.636849</td>\n",
       "      <td>0.071818</td>\n",
       "      <td>0.889676</td>\n",
       "      <td>0.007921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.476470</td>\n",
       "      <td>0.159624</td>\n",
       "      <td>0.896875</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>4.665729</td>\n",
       "      <td>0.081332</td>\n",
       "      <td>0.899014</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.469561</td>\n",
       "      <td>0.151312</td>\n",
       "      <td>0.898664</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>4.666684</td>\n",
       "      <td>0.070762</td>\n",
       "      <td>0.900921</td>\n",
       "      <td>0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.504186</td>\n",
       "      <td>0.143505</td>\n",
       "      <td>0.900346</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>4.703885</td>\n",
       "      <td>0.054146</td>\n",
       "      <td>0.902709</td>\n",
       "      <td>0.001085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.500647</td>\n",
       "      <td>0.138937</td>\n",
       "      <td>0.901695</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>4.716172</td>\n",
       "      <td>0.058116</td>\n",
       "      <td>0.904108</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.519380</td>\n",
       "      <td>0.139424</td>\n",
       "      <td>0.902385</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>4.728211</td>\n",
       "      <td>0.051811</td>\n",
       "      <td>0.904896</td>\n",
       "      <td>0.001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.517151</td>\n",
       "      <td>0.143001</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>4.742375</td>\n",
       "      <td>0.043535</td>\n",
       "      <td>0.905632</td>\n",
       "      <td>0.001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.526006</td>\n",
       "      <td>0.131251</td>\n",
       "      <td>0.904405</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>4.746904</td>\n",
       "      <td>0.043487</td>\n",
       "      <td>0.906950</td>\n",
       "      <td>0.000576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.550568</td>\n",
       "      <td>0.122610</td>\n",
       "      <td>0.905323</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>4.753078</td>\n",
       "      <td>0.041267</td>\n",
       "      <td>0.907845</td>\n",
       "      <td>0.000566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.549789</td>\n",
       "      <td>0.129016</td>\n",
       "      <td>0.906284</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>4.752942</td>\n",
       "      <td>0.043883</td>\n",
       "      <td>0.908812</td>\n",
       "      <td>0.000477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.551259</td>\n",
       "      <td>0.146138</td>\n",
       "      <td>0.907274</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>4.763211</td>\n",
       "      <td>0.037831</td>\n",
       "      <td>0.909853</td>\n",
       "      <td>0.000364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.565002</td>\n",
       "      <td>0.143106</td>\n",
       "      <td>0.907827</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>4.767701</td>\n",
       "      <td>0.041295</td>\n",
       "      <td>0.910405</td>\n",
       "      <td>0.000573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.580086</td>\n",
       "      <td>0.125442</td>\n",
       "      <td>0.908480</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>4.774323</td>\n",
       "      <td>0.038247</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.000553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.577348</td>\n",
       "      <td>0.127091</td>\n",
       "      <td>0.909042</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>4.780973</td>\n",
       "      <td>0.029271</td>\n",
       "      <td>0.911499</td>\n",
       "      <td>0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.588229</td>\n",
       "      <td>0.126207</td>\n",
       "      <td>0.909744</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>4.793153</td>\n",
       "      <td>0.034759</td>\n",
       "      <td>0.912172</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.590179</td>\n",
       "      <td>0.127695</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>4.805176</td>\n",
       "      <td>0.027649</td>\n",
       "      <td>0.912531</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>5.236346</td>\n",
       "      <td>0.219563</td>\n",
       "      <td>0.933773</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>9.384119</td>\n",
       "      <td>0.250718</td>\n",
       "      <td>0.947499</td>\n",
       "      <td>0.000479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>5.240087</td>\n",
       "      <td>0.223228</td>\n",
       "      <td>0.933769</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>9.398485</td>\n",
       "      <td>0.256593</td>\n",
       "      <td>0.947550</td>\n",
       "      <td>0.000496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>5.244044</td>\n",
       "      <td>0.221370</td>\n",
       "      <td>0.933771</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>9.430400</td>\n",
       "      <td>0.279973</td>\n",
       "      <td>0.947579</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>5.250439</td>\n",
       "      <td>0.210332</td>\n",
       "      <td>0.933795</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>9.470796</td>\n",
       "      <td>0.293837</td>\n",
       "      <td>0.947661</td>\n",
       "      <td>0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>5.251846</td>\n",
       "      <td>0.214863</td>\n",
       "      <td>0.933791</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>9.486690</td>\n",
       "      <td>0.297252</td>\n",
       "      <td>0.947710</td>\n",
       "      <td>0.000516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>5.248238</td>\n",
       "      <td>0.206212</td>\n",
       "      <td>0.933791</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>9.526008</td>\n",
       "      <td>0.320082</td>\n",
       "      <td>0.947758</td>\n",
       "      <td>0.000508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>5.250397</td>\n",
       "      <td>0.216609</td>\n",
       "      <td>0.933791</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>9.539476</td>\n",
       "      <td>0.334784</td>\n",
       "      <td>0.947791</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>5.249636</td>\n",
       "      <td>0.215025</td>\n",
       "      <td>0.933793</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>9.553087</td>\n",
       "      <td>0.332614</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.000518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>5.249894</td>\n",
       "      <td>0.214727</td>\n",
       "      <td>0.933795</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>9.576641</td>\n",
       "      <td>0.324458</td>\n",
       "      <td>0.947856</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>5.242765</td>\n",
       "      <td>0.206243</td>\n",
       "      <td>0.933803</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>9.596982</td>\n",
       "      <td>0.324716</td>\n",
       "      <td>0.947894</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>5.244234</td>\n",
       "      <td>0.211469</td>\n",
       "      <td>0.933812</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>9.624060</td>\n",
       "      <td>0.311840</td>\n",
       "      <td>0.947925</td>\n",
       "      <td>0.000493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>5.250445</td>\n",
       "      <td>0.221363</td>\n",
       "      <td>0.933813</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>9.649933</td>\n",
       "      <td>0.293155</td>\n",
       "      <td>0.947961</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>5.260818</td>\n",
       "      <td>0.219132</td>\n",
       "      <td>0.933813</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>9.664223</td>\n",
       "      <td>0.311025</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>0.000473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>5.259716</td>\n",
       "      <td>0.226641</td>\n",
       "      <td>0.933816</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>9.691138</td>\n",
       "      <td>0.300999</td>\n",
       "      <td>0.948048</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>5.259902</td>\n",
       "      <td>0.230028</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>9.713685</td>\n",
       "      <td>0.303136</td>\n",
       "      <td>0.948086</td>\n",
       "      <td>0.000456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>5.258698</td>\n",
       "      <td>0.225291</td>\n",
       "      <td>0.933825</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>9.731862</td>\n",
       "      <td>0.307471</td>\n",
       "      <td>0.948123</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>5.264664</td>\n",
       "      <td>0.234828</td>\n",
       "      <td>0.933836</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>9.734387</td>\n",
       "      <td>0.318647</td>\n",
       "      <td>0.948168</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>5.275080</td>\n",
       "      <td>0.233519</td>\n",
       "      <td>0.933835</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>9.759053</td>\n",
       "      <td>0.319013</td>\n",
       "      <td>0.948197</td>\n",
       "      <td>0.000445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>5.268693</td>\n",
       "      <td>0.229953</td>\n",
       "      <td>0.933839</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>9.775638</td>\n",
       "      <td>0.320280</td>\n",
       "      <td>0.948223</td>\n",
       "      <td>0.000439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>5.269096</td>\n",
       "      <td>0.249718</td>\n",
       "      <td>0.933846</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>9.803192</td>\n",
       "      <td>0.313325</td>\n",
       "      <td>0.948253</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>5.271813</td>\n",
       "      <td>0.250178</td>\n",
       "      <td>0.933853</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>9.836400</td>\n",
       "      <td>0.320772</td>\n",
       "      <td>0.948294</td>\n",
       "      <td>0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>5.269676</td>\n",
       "      <td>0.247371</td>\n",
       "      <td>0.933856</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>9.836791</td>\n",
       "      <td>0.323585</td>\n",
       "      <td>0.948333</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>5.262793</td>\n",
       "      <td>0.256417</td>\n",
       "      <td>0.933858</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>9.853402</td>\n",
       "      <td>0.295439</td>\n",
       "      <td>0.948372</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>5.259254</td>\n",
       "      <td>0.250975</td>\n",
       "      <td>0.933854</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>9.861957</td>\n",
       "      <td>0.296531</td>\n",
       "      <td>0.948391</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>5.259828</td>\n",
       "      <td>0.245470</td>\n",
       "      <td>0.933855</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>9.892492</td>\n",
       "      <td>0.312435</td>\n",
       "      <td>0.948419</td>\n",
       "      <td>0.000417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>5.257069</td>\n",
       "      <td>0.244498</td>\n",
       "      <td>0.933850</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>9.901814</td>\n",
       "      <td>0.312334</td>\n",
       "      <td>0.948450</td>\n",
       "      <td>0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>5.257608</td>\n",
       "      <td>0.242642</td>\n",
       "      <td>0.933859</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>9.923688</td>\n",
       "      <td>0.325282</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>5.264140</td>\n",
       "      <td>0.241960</td>\n",
       "      <td>0.933859</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>9.929613</td>\n",
       "      <td>0.326536</td>\n",
       "      <td>0.948502</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>5.268199</td>\n",
       "      <td>0.237741</td>\n",
       "      <td>0.933861</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>9.953365</td>\n",
       "      <td>0.332304</td>\n",
       "      <td>0.948539</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>5.269483</td>\n",
       "      <td>0.234819</td>\n",
       "      <td>0.933862</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>9.980575</td>\n",
       "      <td>0.330185</td>\n",
       "      <td>0.948560</td>\n",
       "      <td>0.000434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-ams@0.15-mean  test-ams@0.15-std  test-auc-mean  test-auc-std  \\\n",
       "0              3.909543           0.097884       0.842198      0.001194   \n",
       "1              4.236945           0.181689       0.854647      0.010645   \n",
       "2              4.242105           0.121839       0.859342      0.011050   \n",
       "3              4.307576           0.093919       0.869186      0.002418   \n",
       "4              4.357200           0.120168       0.870837      0.003634   \n",
       "5              4.382403           0.120171       0.873463      0.003857   \n",
       "6              4.392322           0.120804       0.877322      0.002670   \n",
       "7              4.420645           0.149904       0.878349      0.002824   \n",
       "8              4.435181           0.163421       0.879478      0.002516   \n",
       "9              4.452390           0.158298       0.879849      0.002231   \n",
       "10             4.447331           0.156880       0.880632      0.002371   \n",
       "11             4.454632           0.154264       0.881006      0.002305   \n",
       "12             4.447309           0.177235       0.884396      0.004628   \n",
       "13             4.418153           0.166041       0.884807      0.005140   \n",
       "14             4.465543           0.156981       0.887643      0.007155   \n",
       "15             4.476470           0.159624       0.896875      0.000793   \n",
       "16             4.469561           0.151312       0.898664      0.001623   \n",
       "17             4.504186           0.143505       0.900346      0.002144   \n",
       "18             4.500647           0.138937       0.901695      0.002011   \n",
       "19             4.519380           0.139424       0.902385      0.001741   \n",
       "20             4.517151           0.143001       0.903084      0.001819   \n",
       "21             4.526006           0.131251       0.904405      0.001634   \n",
       "22             4.550568           0.122610       0.905323      0.001752   \n",
       "23             4.549789           0.129016       0.906284      0.001692   \n",
       "24             4.551259           0.146138       0.907274      0.001533   \n",
       "25             4.565002           0.143106       0.907827      0.001319   \n",
       "26             4.580086           0.125442       0.908480      0.001278   \n",
       "27             4.577348           0.127091       0.909042      0.001318   \n",
       "28             4.588229           0.126207       0.909744      0.001380   \n",
       "29             4.590179           0.127695       0.910012      0.001592   \n",
       "..                  ...                ...            ...           ...   \n",
       "286            5.236346           0.219563       0.933773      0.000749   \n",
       "287            5.240087           0.223228       0.933769      0.000757   \n",
       "288            5.244044           0.221370       0.933771      0.000756   \n",
       "289            5.250439           0.210332       0.933795      0.000759   \n",
       "290            5.251846           0.214863       0.933791      0.000744   \n",
       "291            5.248238           0.206212       0.933791      0.000758   \n",
       "292            5.250397           0.216609       0.933791      0.000763   \n",
       "293            5.249636           0.215025       0.933793      0.000766   \n",
       "294            5.249894           0.214727       0.933795      0.000764   \n",
       "295            5.242765           0.206243       0.933803      0.000757   \n",
       "296            5.244234           0.211469       0.933812      0.000766   \n",
       "297            5.250445           0.221363       0.933813      0.000773   \n",
       "298            5.260818           0.219132       0.933813      0.000770   \n",
       "299            5.259716           0.226641       0.933816      0.000780   \n",
       "300            5.259902           0.230028       0.933826      0.000781   \n",
       "301            5.258698           0.225291       0.933825      0.000781   \n",
       "302            5.264664           0.234828       0.933836      0.000782   \n",
       "303            5.275080           0.233519       0.933835      0.000774   \n",
       "304            5.268693           0.229953       0.933839      0.000776   \n",
       "305            5.269096           0.249718       0.933846      0.000780   \n",
       "306            5.271813           0.250178       0.933853      0.000788   \n",
       "307            5.269676           0.247371       0.933856      0.000790   \n",
       "308            5.262793           0.256417       0.933858      0.000792   \n",
       "309            5.259254           0.250975       0.933854      0.000798   \n",
       "310            5.259828           0.245470       0.933855      0.000800   \n",
       "311            5.257069           0.244498       0.933850      0.000798   \n",
       "312            5.257608           0.242642       0.933859      0.000792   \n",
       "313            5.264140           0.241960       0.933859      0.000788   \n",
       "314            5.268199           0.237741       0.933861      0.000797   \n",
       "315            5.269483           0.234819       0.933862      0.000798   \n",
       "\n",
       "     train-ams@0.15-mean  train-ams@0.15-std  train-auc-mean  train-auc-std  \n",
       "0               4.012500            0.154041        0.843308       0.000947  \n",
       "1               4.323271            0.080539        0.855859       0.010494  \n",
       "2               4.332760            0.110162        0.860942       0.011385  \n",
       "3               4.382181            0.072945        0.870628       0.001795  \n",
       "4               4.451794            0.040777        0.872648       0.003224  \n",
       "5               4.487194            0.044543        0.875088       0.002343  \n",
       "6               4.497964            0.058888        0.878971       0.001477  \n",
       "7               4.523607            0.020063        0.879917       0.001237  \n",
       "8               4.532700            0.029385        0.881286       0.001085  \n",
       "9               4.581649            0.046796        0.881827       0.001204  \n",
       "10              4.571364            0.072250        0.882651       0.001280  \n",
       "11              4.595838            0.066355        0.883039       0.001316  \n",
       "12              4.608398            0.073009        0.886500       0.006251  \n",
       "13              4.591107            0.099510        0.886790       0.006548  \n",
       "14              4.636849            0.071818        0.889676       0.007921  \n",
       "15              4.665729            0.081332        0.899014       0.001609  \n",
       "16              4.666684            0.070762        0.900921       0.000932  \n",
       "17              4.703885            0.054146        0.902709       0.001085  \n",
       "18              4.716172            0.058116        0.904108       0.001449  \n",
       "19              4.728211            0.051811        0.904896       0.001007  \n",
       "20              4.742375            0.043535        0.905632       0.001029  \n",
       "21              4.746904            0.043487        0.906950       0.000576  \n",
       "22              4.753078            0.041267        0.907845       0.000566  \n",
       "23              4.752942            0.043883        0.908812       0.000477  \n",
       "24              4.763211            0.037831        0.909853       0.000364  \n",
       "25              4.767701            0.041295        0.910405       0.000573  \n",
       "26              4.774323            0.038247        0.911011       0.000553  \n",
       "27              4.780973            0.029271        0.911499       0.000574  \n",
       "28              4.793153            0.034759        0.912172       0.000651  \n",
       "29              4.805176            0.027649        0.912531       0.000606  \n",
       "..                   ...                 ...             ...            ...  \n",
       "286             9.384119            0.250718        0.947499       0.000479  \n",
       "287             9.398485            0.256593        0.947550       0.000496  \n",
       "288             9.430400            0.279973        0.947579       0.000491  \n",
       "289             9.470796            0.293837        0.947661       0.000497  \n",
       "290             9.486690            0.297252        0.947710       0.000516  \n",
       "291             9.526008            0.320082        0.947758       0.000508  \n",
       "292             9.539476            0.334784        0.947791       0.000513  \n",
       "293             9.553087            0.332614        0.947826       0.000518  \n",
       "294             9.576641            0.324458        0.947856       0.000507  \n",
       "295             9.596982            0.324716        0.947894       0.000502  \n",
       "296             9.624060            0.311840        0.947925       0.000493  \n",
       "297             9.649933            0.293155        0.947961       0.000483  \n",
       "298             9.664223            0.311025        0.948000       0.000473  \n",
       "299             9.691138            0.300999        0.948048       0.000470  \n",
       "300             9.713685            0.303136        0.948086       0.000456  \n",
       "301             9.731862            0.307471        0.948123       0.000437  \n",
       "302             9.734387            0.318647        0.948168       0.000437  \n",
       "303             9.759053            0.319013        0.948197       0.000445  \n",
       "304             9.775638            0.320280        0.948223       0.000439  \n",
       "305             9.803192            0.313325        0.948253       0.000436  \n",
       "306             9.836400            0.320772        0.948294       0.000426  \n",
       "307             9.836791            0.323585        0.948333       0.000424  \n",
       "308             9.853402            0.295439        0.948372       0.000405  \n",
       "309             9.861957            0.296531        0.948391       0.000409  \n",
       "310             9.892492            0.312435        0.948419       0.000417  \n",
       "311             9.901814            0.312334        0.948450       0.000422  \n",
       "312             9.923688            0.325282        0.948471       0.000427  \n",
       "313             9.929613            0.326536        0.948502       0.000430  \n",
       "314             9.953365            0.332304        0.948539       0.000432  \n",
       "315             9.980575            0.330185        0.948560       0.000434  \n",
       "\n",
       "[316 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEXCAYAAACgUUN5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+/+dkpQDWh1jVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dea4250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_estimators = cvresult.shape[0]\n",
    "\n",
    "from matplotlib import pyplot\n",
    "# plot\n",
    "test_means = cvresult['test-ams@0.15-mean']\n",
    "test_stds = cvresult['test-ams@0.15-std'] \n",
    "        \n",
    "train_means = cvresult['train-ams@0.15-mean']\n",
    "train_stds = cvresult['train-ams@0.15-std'] \n",
    "\n",
    "x_axis = range(0, n_estimators)\n",
    "pyplot.errorbar(x_axis, test_means, yerr=test_stds ,label='Test')\n",
    "pyplot.errorbar(x_axis, train_means, yerr=train_stds ,label='Train')\n",
    "pyplot.title(\"HiggsBoson n_estimators vs ams@0.15\")\n",
    "pyplot.xlabel( 'n_estimators' )\n",
    "pyplot.ylabel( 'ams@0.15' )\n",
    "pyplot.savefig( 'HiggsBoson_estimators.png' )\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the algorithm on the data, cv 函数没有refit步骤\n",
    "#alg.fit(X_train, y_train, eval_metric='ams@0.15')\n",
    "print ('train model using the best parameters by cv ... ')\n",
    "bst = xgb.train( param, dtrain, n_estimators );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train finished\n"
     ]
    }
   ],
   "source": [
    "# save out model\n",
    "bst.save_model('higgs_cv.model')\n",
    "\n",
    "print ('train finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试过程（higgs-pred.py）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# make prediction\n",
    "import numpy as np\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取测试数据和训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading from csv \n"
     ]
    }
   ],
   "source": [
    "# path to where the data lies\n",
    "dpath = './data/'\n",
    "\n",
    "modelfile = 'higgs_cv.model'\n",
    "outfile = 'higgs.pred.csv'\n",
    "# make top 15% as positive\n",
    "threshold_ratio = 0.15\n",
    "\n",
    "# load in test, directly use numpy\n",
    "dtest = np.loadtxt( dpath+'/higgsboson_test.csv', delimiter=',', skiprows=1 )\n",
    "data   = dtest[:,1:31]\n",
    "idx = dtest[:,0]\n",
    "\n",
    "print ('finish loading from csv ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost测试环境准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试数据导入DMatrix\n",
    "模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgmat = xgb.DMatrix( data, missing = -999.0 )\n",
    "bst = xgb.Booster({'nthread':8}, model_file = modelfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ypred = bst.predict( xgmat )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试结果整理，写入结果提交文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished writing into prediction file\n"
     ]
    }
   ],
   "source": [
    "res  = [ ( int(idx[i]), ypred[i] ) for i in range(len(ypred)) ]\n",
    "\n",
    "rorder = {}\n",
    "for k, v in sorted( res, key = lambda x:-x[1] ):\n",
    "    rorder[ k ] = len(rorder) + 1\n",
    "\n",
    "# write out predictions\n",
    "ntop = int( threshold_ratio * len(rorder ) )\n",
    "fo = open(outfile, 'w')\n",
    "nhit = 0\n",
    "ntot = 0\n",
    "fo.write('EventId,RankOrder,Class\\n')\n",
    "for k, v in res:\n",
    "    if rorder[k] <= ntop:\n",
    "        lb = 's'\n",
    "        nhit += 1\n",
    "    else:\n",
    "        lb = 'b'\n",
    "    # change output rank order to follow Kaggle convention\n",
    "    fo.write('%s,%d,%s\\n' % ( k,  len(rorder)+1-rorder[k], lb ) )\n",
    "    ntot += 1\n",
    "fo.close()\n",
    "\n",
    "print ('finished writing into prediction file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
